ajouter XSLT pour em. Sanderson ; éditer schéma + guide

---

XXX use relative imports or instead "import dharma.xx"?

Make backups of the dbs!

---

must keep track of older commits to aid fixing invalid references that were
valid at some point

table 'commits'
(primary key: repo name + commit hash)
* repo name
* commit hash (full)
* commit date
* push date (if unknown, use the commit date)
should have a way to extract easily the latest commit hash of a repo given its
name

table 'texts' for enumerating texts, containing:
(primary key: text name + repo name + commit hash)
XXX how to check that texts are unique at anny given point)
* text name without the extension viz. text id
* repo name + commit hash (for joining with 'commits')
* full path of the text within the repo, without a leading slash, and including
  the file name with the XML extension.
* location of the corresponding HTML file within the repo
With this information, we can extract a particular version of a text from the
appropriate repo, we can also link to it on github.

table 'validation' for error messages; this must be separated from texts
because there can be several different validations for a single file
(primary key: text name + repo name + commit hash)
* text name without the extension
* repo name + commit hash
* hash of the commit of my repo that was used for validation. note that we must
  add schemas to our repo
* valid flag (null if unknown, otherwise true or false)
* if invalid, summary of errors in some json format
* when we validated the file

Should also have a table where we hold the names of all XML files (texts as
well as metadata files), to make sure texts are unique.

pragma foreign_keys = on;
pragma journal_mode = wal;
pragma page_size = 8192;

create table commits(
   repo text,
   commit_hash text,
   commit_date integer,
   push_date integer,
   primary key(repo, commit_hash)
);

create table texts(
   name text,
   repo text,
   commit_hash text,
   xml_path text,
   html_path text,
   primary key(name, repo, commit_hash),
   foreign key(repo, commit_hash) references commits(repo, commit_hash)
);

create table validation(
   name text,
   repo text,
   commit_hash text,
   code_hash text,
   valid boolean,
   errors json,
   primary key(name, repo, commit_hash),
   foreign key(name, repo, commit_hash) references texts(name, repo,
      commit_hash),
);

---

# Validation and change detection

Must be able to show all most recent files together with their validation
status. Be able to sort by repo, validation status, last mod date, name (add
up/down arrows to each of these columns).

When a repo is updated, we gather all texts within the repo, delete records
from 'texts' where repo=the repo, and insert newer records.

I don't think it's necessary to do things at the file level. Even moreso
because files can be deleted/added. It's simpler to proceed repo per repo. Each
time we have a github push, revalidate everything in the corresponding repo.

The simpler method is to run an extra python process for all the stuff related
to updates, and send it some basic commands. This way we can test things
easily. Should have a single queue, this is sufficient given the rythm of
updates. We can also send it commands from cron scripts, etc. This process can
remain connected to the commit log and decide what to do. Though we shouldn't
rely on what github says, they make no promise on the API stability. We just
need to know that repository X has been modified, then we run our stuff.

Every now and then, we should do a global update in case we missed events or if
github didn't send them to us. At a minimum, should do a global update at
startup.

The only data we need to pass to the update script is the name of the updated
repo (or "all" for updating all repos). When we see that a repo changed, we
update it locally with git and see if stuff actually changed and what changed.
If the queue overflows, should just clear it and then run a global update.
Should just use a socket, without extra buffer management (but set a reasonable
buf size), and then send it repo names, one per line. The write side must be
non-blocking. On the read side, we deduplicate stuff (and take care of "all")
to reduce the amount of stuff to do.

Must put the various repos somewhere not in the main repo viz. don't track them
at all, don't use submodules. Pass the location of the "repos" dir with an env
var or something, like for the dbs directory. Maybe create a python config file
to clarify things.

---

An an



---

Should have a useful validation process, something that gives us guarantees
about the ability to process files and to do something with them. TEI and
EpiDoc are so generic that it's not possible to have useful tools.

Set up some tooling for validating XML texts in the repos, with the existing
Schematron files and maybe some extra scripts for things that need to be tested
with real programming. Also validate the bibliographic entries they reference
at the same time. This should all be doable at the command line and be quick
enough (run as a service ?)

Should use a layered validation process: first test basic textual conformance
(encoding, etc.), then XML, then TEI, then EpiDoc, then finally our schemas.
No, each file should be tested against a single schema for the "normal"
validation process; we should test other schemas too, but only internally, for
consistency. If your schemas are correct, then all our files should validate
against TEI and EpiDoc.

Test the validation process. We must ensure that the files that are considered
valid now remain so when we update the schema, etc. It must be kept in mind
that some people won't modify their files anymore (left the project, etc.) and
that we don't have access to all files, so changing things in a
backward-incompatible way is _not_ OK.

Need to normalize:

   <persName ref="part:jodo">
   <forename>John</forename>
   <surname>Doe</surname>
   </persName>

We already have a mapping in
https://github.com/erc-dharma/project-documentation/blob/master/DHARMA_idListMembers_v01.xml
so no reason to duplicate the data.

Plenty of useless markup we can systematically remove and patch later on.

It's pointless to use ODD as a schema format. The format sucks. We only need
RNG. RNG is sane. We need it for help screens and autocompletion in oXygen and
co.; if we don't use it directly, we must at least be able to generate a RNG
file from our schema.

* Possible to annotate nodes in relaxng. It interprets lines starting with '##'
  as documentation and outputs it in the generated XML. We can use that.
* In any case, should have both the schema and the processing code in the same
  file, so that we don't have to do the same checks several times.

Maybe the best way to write the grammar is to have both a normal XML schema
that matches nodes and an abstract grammar for defining actions. The abstract
grammar would be non-deterministic, we can't formalize everything.

For the grammar, could use https://we-like-parsers.github.io/pegen
Can probably convert RNG<->PEG without loss of information and automate this.
But need to add a special syntax for tags, attributes, and content types viz.

   text
   xsd:anyURI
   xsd:boolean
   xsd:decimal
   xsd:integer
   xsd:NCName
   xsd:NMTOKEN

Should have:

   tag.div, ...
   attr.type, ...
   data.text, ...

# Conversion

Set up a XML->(almost plain) text conversion system, a readable representation
of the internal representation we'll use for search, the simplest one for now.
(Later on we'll need a more complicated search representation to support
highlighting search results in full documents.) At this stage don't try to
process files that were rejected as invalid beforehand. We assume the files are
correct in the conversion code, we don't attempt to validate things here,
otherwise the conversion code will be too complicated and unmaintainable. This
conversion code will be used at search time for highlighting search results, so
it must be fast; probably should write it in go as well as all the
server-related stuff.

Add a XML->HTML conversion capability to our basic conversion system. Should
use an intermediate representation, but which one? Could preprocess XML files
to keep just what we need in a convenient format, then generate the HTML from
there. We'll need to generate the HTML at search time, so should be fast and
written in go.

The best solution for the intermediate representation is to use a custom
bytecode and to evaluate it differently depending on what we want to generate.
We should store this representation in the db. The main advantage of using this
representation is to avoid having to manipulate hairy structures.

Add a XML->LaTeX conversion. Need not be fast, can be done in python, we'll
pregenerate stuff.


# Extra

Figure out something for simplifying the encoding process. We already have
Schematron quick fixes, but it'd also be useful to write some routines that
fill automatically stuff that can be done mechanically (license, etc.) or
propose various transformations. Maybe have a way to accept/reject a given
transformation. Does SQF support this kind of thing? If so, does it also
support calling external scripts, etc.?
