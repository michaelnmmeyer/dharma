XXX for date of last update of texts db, use not the last time we actually
changed the db, but when we tried to do that.

== Transform

When we have stuff like:

	text 'foo'
	html '<i>'
	text 'bar'
	html '</i>'
	text 'baz'

How to proceed for highlighting "foo bar", viz. when the matched text crosses
an HTML tag? It's clunky.

The simplest method is maybe to store the raw text separately as a contiguous
string where fields are separated with some character, and to have pointers to
the raw text in the bytecode. This makes it convenient to match the text with
or without taking into account the structure. We need at least two separate
buffers: one for the raw text + structure, the other for HTML stuff.

For generating the HTML. We keep track of match offsets in an array and, when
generating the HTML, we highlight portions of texts that need to be. There is
no risk of overlap since the HTML is stored separately from the plain text.
Extracting snippets is still somewhat annoying.

What should we do with <foreign> passages, to support language-specific search?


----------------------------

// delete processing instructions + <?xml...
<TEI xml:lang="eng" xmlns="http://www.tei-c.org/ns/1.0">
  // delete these TEI @ (other @ are significant)
  <teiHeader>
    <fileDesc>
      <titleStmt>...</titleStmt>
      <editor ref="part:anac">
        // delete contents; but only when we have filedesc/editor! there is
        // also an editor field in the bibliography
      </editor>
      <respStmt>
        <resp>Creation of the file</resp>
          <persName ref="part:kuch">
            // Delete contents
          </persName>
	  <persName>
            ...
          </persName>
          ...
        </resp>
      </respStmt>
      <publicationStmt>
        <authority>DHARMA</authority> // not always DHARMA! check
        // Within this, only keep <pubPlace>, delete everything else
        <pubPlace>KEEP THIS</pubPlace>
      </publicationStmt>
      <sourceDesc>
        // Only keep if there is non-blank text. We have bibliographies here biblFull
      </sourceDesc>
    </filedesc>
    <encodingDesc>...</encodingDesc> // Delete completely
    <revisionDesc>...</revisionDesc> // Delete completely
  </teiHeader>
  <text xml:space="preserve"> // delete this @

---


ajouter XSLT pour em. Sanderson ; éditer schéma + guide



---


# Validation

Should have a useful validation process, something that gives us guarantees
about the ability to process files and to do something with them. TEI and
EpiDoc are so generic that it's not possible to have useful tools.

Set up some tooling for validating XML texts in the repos, with the existing
Schematron files and maybe some extra scripts for things that need to be tested
with real programming. Also validate the bibliographic entries they reference
at the same time. This should all be doable at the command line and be quick
enough (run as a service ?)

Should use a layered validation process: first test basic textual conformance
(encoding, etc.), then XML, then TEI, then EpiDoc, then finally our schemas.
No, each file should be tested against a single schema for the "normal"
validation process; we should test other schemas too, but only internally, for
consistency. If your schemas are correct, then all our files should validate
against TEI and EpiDoc.

Test the validation process. We must ensure that the files that are considered
valid now remain so when we update the schema, etc. It must be kept in mind
that some people won't modify their files anymore (left the project, etc.) and
that we don't have access to all files, so changing things in a
backward-incompatible way is not OK, unless we DIO.

Need to normalize:

   <persName ref="part:jodo">
   <forename>John</forename>
   <surname>Doe</surname>
   </persName>

We already have a mapping in
https://github.com/erc-dharma/project-documentation/blob/master/DHARMA_idListMembers_v01.xml
so no reason to duplicate the data. Must use the notation x:y for referring to
xml:id in files. To follow existing conventions allow x to be not a file name
(viz. name without the extension), but optionally a custom shorter prefix,
which we add to a global map. The prefix could even refer to a set of files,
not to a single file, but it's more annoying to check where it comes from. For
now we have the following prefixes:

	bib		OK
	part		=project-documentation/DHARMA_idListMembers_v01.xml

	Plenty of other prefixes! look for @xml:id, @ref, and the ptr mess in
	TEI. Should use a single system.

Plenty of useless markup we can systematically remove and patch later on.

It's pointless to use ODD as a schema format. The format sucks. We only need
RNG. RNG is sane. We need it for help screens and autocompletion in oXygen and
co.; if we don't use it directly, we must at least be able to generate a RNG
file from our schema.

* Possible to annotate nodes in relaxng. It interprets lines starting with '##'
  as documentation and outputs it in the generated XML. We can use that.
* In any case, should have both the schema and the processing code in the same
  file, so that we don't have to do the same checks several times.

Maybe the best way to write the grammar is to have both a normal XML schema
that matches nodes and an abstract grammar for defining actions. The abstract
grammar would be non-deterministic, we can't formalize everything.

For the grammar, could use https://we-like-parsers.github.io/pegen
Can probably convert RNG<->PEG without loss of information and automate this.
But need to add a special syntax for tags, attributes, and content types viz.

   text
   xsd:anyURI
   xsd:boolean
   xsd:decimal
   xsd:integer
   xsd:NCName
   xsd:NMTOKEN

Should have:

   tag.div, ...
   attr.type, ...
   data.text, ...

# Conversion

Set up a XML->(almost plain) text conversion system, a readable representation
of the internal representation we'll use for search, the simplest one for now.
(Later on we'll need a more complicated search representation to support
highlighting search results in full documents.) At this stage don't try to
process files that were rejected as invalid beforehand. We assume the files are
correct in the conversion code, we don't attempt to validate things here,
otherwise the conversion code will be too complicated and unmaintainable. This
conversion code will be used at search time for highlighting search results, so
it must be fast; probably should write it in go as well as all the
server-related stuff.

Add a XML->HTML conversion capability to our basic conversion system. Should
use an intermediate representation, but which one? Could preprocess XML files
to keep just what we need in a convenient format, then generate the HTML from
there. We'll need to generate the HTML at search time, so should be fast and
written in go.

The best solution for the intermediate representation is to use a custom
bytecode and to evaluate it differently depending on what we want to generate.
We should store this representation in the db. The main advantage of using this
representation is to avoid having to manipulate hairy structures.

Add a XML->LaTeX conversion. Need not be fast, can be done in python, we'll
pregenerate stuff.


# Extra

Figure out something for simplifying the encoding process. We already have
Schematron quick fixes, but it'd also be useful to write some routines that
fill automatically stuff that can be done mechanically (license, etc.) or
propose various transformations. Maybe have a way to accept/reject a given
transformation. Does SQF support this kind of thing? If so, does it also
support calling external scripts, etc.?
