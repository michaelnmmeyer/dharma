== Plans

Make backups of the dbs!


# Validation

Must be able to show all most recent files together with their validation
status. Be able to sort by repo, validation status, last mod date, name (add
up/down arrows to each of these columns).

For each text we validate, must keep track of:

* file hash
* full path, relative to the root of the repo
* repo name
* commit hash of the repo the file belongs to
* name of the schema against which it's validated and commit hash of
  the corresponding project-documentation
* commit hash of my repo (the actual validation code)
* when we validated the file

I don't think it's necessary to do things at the file level. Even moreso
because files can be deleted/added. It's simpler to proceed repo per repo. Each
time we have a github push, revalidate everything in the corresponding repo.

Must hold a lock while revalidating stuff. Does bottle use multiple threads? or
processes? For flask, see
https://flask.palletsprojects.com/en/2.3.x/patterns/celery/

The simpler method is to run an extra python process for all the stuff related
to updates, and send it some basic commands. This way we can test things
easily. Should have a single queue, this is sufficient given the rythm of
updates. We can also send it commands from cron scripts, etc. This process can
remain connected to the commit log and decide what to do. Though we shouldn't
rely on what github says, they make no promise on the API stability. We just
need to know that repository X has been modified, then we run our stuff.

Every now and then, we should do a global update in case we missed events or if
github didn't send them to us. At a minimum, should do a global update at
startup.

The only data we need to pass to the update script is the name of the updated
repo (or "all" for updating all repos). When we see that a repo changed, we
update it locally with git and see if stuff actually changed and what changed.
If the queue overflows, should just clear it and then run a global update.
Should just use a socket, without extra buffer management (but set a reasonable
buf size), and then send it repo names, one per line. The write side must be
non-blocking. On the read side, we deduplicate stuff (and take care of "all")
to reduce the amount of stuff to do.

Must put the various repos somewhere not in the main repo viz. don't track them
at all, don't use submodules. Pass the location of the "repos" dir with an env
var or something, like for the dbs directory. Maybe create a python config file
to clarify things.

---

Should have a useful validation process, something that gives us guarantees
about the ability to process files and to do something with them. TEI and
EpiDoc are so generic that it's not possible to have useful tools.

Set up some tooling for validating XML texts in the repos, with the existing
Schematron files and maybe some extra scripts for things that need to be tested
with real programming. Also validate the bibliographic entries they reference
at the same time. This should all be doable at the command line and be quick
enough (run as a service ?)

Should use a layered validation process: first test basic textual conformance
(encoding, etc.), then XML, then TEI, then EpiDoc, then finally our schemas.
No, each file should be tested against a single schema for the "normal"
validation process; we should test other schemas too, but only internally, for
consistency. If your schemas are correct, then all our files should validate
against TEI and EpiDoc.

Test the validation process. We must ensure that the files that are considered
valid now remain so when we update the schema, etc. It must be kept in mind
that some people won't modify their files anymore (left the project, etc.) and
that we don't have access to all files, so changing things in a
backward-incompatible way is _not_ OK.

Need to normalize:

   <persName ref="part:jodo">
   <forename>John</forename>
   <surname>Doe</surname>
   </persName>

We already have a mapping in
https://github.com/erc-dharma/project-documentation/blob/master/DHARMA_idListMembers_v01.xml
so no reason to duplicate the data.

Plenty of useless markup we can systematically remove and patch later on.

It's pointless to use ODD as a schema format. The format sucks. We only need
RNG. RNG is sane. We need it for help screens and autocompletion in oXygen and
co.; if we don't use it directly, we must at least be able to generate a RNG
file from our schema.

* Possible to annotate nodes in relaxng. It interprets lines starting with '##'
  as documentation and outputs it in the generated XML. We can use that.
* In any case, should have both the schema and the processing code in the same
  file, so that we don't have to do the same checks several times.

Maybe the best way to write the grammar is to have both a normal XML schema
that matches nodes and an abstract grammar for defining actions. The abstract
grammar would be non-deterministic, we can't formalize everything.

For the grammar, could use https://we-like-parsers.github.io/pegen
Can probably convert RNG<->PEG without loss of information and automate this.
But need to add a special syntax for tags, attributes, and content types viz.

   text
   xsd:anyURI
   xsd:boolean
   xsd:decimal
   xsd:integer
   xsd:NCName
   xsd:NMTOKEN

Should have:

   tag.div, ...
   attr.type, ...
   data.text, ...

# Conversion

Set up a XML->(almost plain) text conversion system, a readable representation
of the internal representation we'll use for search, the simplest one for now.
(Later on we'll need a more complicated search representation to support
highlighting search results in full documents.) At this stage don't try to
process files that were rejected as invalid beforehand. We assume the files are
correct in the conversion code, we don't attempt to validate things here,
otherwise the conversion code will be too complicated and unmaintainable. This
conversion code will be used at search time for highlighting search results, so
it must be fast; probably should write it in go as well as all the
server-related stuff.

Add a XML->HTML conversion capability to our basic conversion system. Should
use an intermediate representation, but which one? Could preprocess XML files
to keep just what we need in a convenient format, then generate the HTML from
there. We'll need to generate the HTML at search time, so should be fast and
written in go.

The best solution for the intermediate representation is to use a custom
bytecode and to evaluate it differently depending on what we want to generate.
We should store this representation in the db. The main advantage of using this
representation is to avoid having to manipulate hairy structures.

Add a XML->LaTeX conversion. Need not be fast, can be done in python, we'll
pregenerate stuff.


# Extra

Figure out something for simplifying the encoding process. We already have
Schematron quick fixes, but it'd also be useful to write some routines that
fill automatically stuff that can be done mechanically (license, etc.) or
propose various transformations. Maybe have a way to accept/reject a given
transformation. Does SQF support this kind of thing? If so, does it also
support calling external scripts, etc.?
