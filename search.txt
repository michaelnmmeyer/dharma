need to restructure the documents table. depends on how we're going to access
the data for search. we at least need to store the patched internal document,
because it takes time to process. no need to store there file-related data that
can be fetched from elsewhere in the db, we can construct an "extended" xml
document on-the-fly quickly enough.

## Query evaluation

for now focus on searching logical.

### snippets

need to determine how to select snippets. must try to select whole semantic
units. start with selecting paragraphs or verses. if a paragraph/verse is too
long, split on dandas and abbreviate segments that do not contain matches with
'[...]', proceeding from the longest segment to the shortest. in another search
tool, i used a minimum length of context (counting in phrases, but might also
count clusters). should do that here as well.

for simplicity, try to select complete (balanced) block elements. but also need
to cap the length of a snippet (or the cumulative length of all snippets), to
avoid degenerate cases with overlong paragraphs. we can abbreviate a block with
'[...]' in appropriate spots. will need to define rules for deciding where
exactly we can split a block.

### highlighting

need a query tree for query evaluation. it should use cursor objects that work
on the searchable text representation. for translating search offsets into
original offsets, we need a tree cursor that iterates over child nodes of a
given field while transforming it to the normalized text, char by char. the role
of a tree cursor is to translate a search offset into a display offset. if we
have different ways to normalize each field, we will need to allocate as many
tree cursors, and we will need to merge their intervals.

how to represent an XML tree offset? we need to be able to merge adjacent and
overlapping intervals. so we must use offsets that map to the string that will
be generated. to make things simpler, could gather in an array the output
string. the array would hold strings, possibly also <couple> elements containing
<display> and <search> element (if the beginning of <search> matches, we select
the whole <display> element, no partial highlighting because impossible).

... we should just integer offsets (no objects), and do a final pass on the tree
to select the corresponding segments. is that enough?

once we've delimited matches in the tree, pass offsets to some generic function
that will figure out if it can just highlight the stuff in a single go or if it
should break it into several portions (because the match crosses a paragraph, a
list element, etc.).

what should the tree cursor look like? to make things simpler, it should iterate
over an array of (string) nodes. then we can implement a seek() method. problem
with the generator is that we will mess up iteration if we modify the tree
immediately.

ideally, the stuff for normalizing strings should advance by arbitrary intervals
(so that we can use a lexer later on).












========

for converting the internal representation to the search one. watch out for @break, which should only occur on <l> elements and on milestones




===

maybe simplest: store separately the raw text and the structure of the document, to make it easy to do regex matching against the raw text, without having to modify TRE. in any case, it's probably the best of course of action.

we thus would have:

* the raw text, as a single string

* a series of pointers arrays, one for each text span we might be interested in.
to save space, might want to use varints and delta-encode the array. we might be
interested in: cluster boundaries, alnum seq boundaries ("words" or "compound
members"), page/line/cell, prose/verse units.

* an offset map to convert a text offset to a location in the internal XML. in
fact it's not really needed, would likely be simpler to just reprocess the XML
and annotate interesting segments as we go.

concerning the matching behaviour:

watch out for regexes that would match the empty string. simplest method maybe: check if the regex matches the empty string before actually searching the db. if it does, inform the user and don't search anything. but we should still return all documents in the collection.

we should probably be non-greedy by default. makes more sense than the reverse in a FTS search context. unless we restrict regex matching to a single cluster: in this case greedy matching should be the norm, because we would be trying to match a whole cluster.

	AB<new para>B

and we match the regex 'A.*B', TRE will return the longest match "AB B"; but if we filter this match out because of <new para>, we're left with no match at all. making matching non-greedy would not address this. we might want e.g. to find occurrences of 'A.*B' that cover at least 3 paragraphs. but non-greedy seems better anyway, so do that. ideally, need to add custom zero-width assertions, but need modifs to tre to allow that.

maybe we should make repetition operators not cross spaces per default, so they would match at most one cluster. would be cleaner in the general case. then the rest of the matching would be performed with normal FTS cursors (phrase search).

===

need to make displays distinctions for the display, but do we also need it for search?

in fact, not exactly. for a start, search only needs to know logical and physical, not full. for sic/corr and orig/reg and abbrevs (am and ex), we do need two versions of the text.

	physical
		hide corr, but only if paired with sic
		hide reg, but only if paired with orig
		hide ex (abbreviation expansion)

	logical
		hide sic, but only if paired with corr
		hide orig, but only if paired with reg
		hide am (abbreviation mark)

should use special characters for representing boundaries (start/end of cluster or line or page or verse or para). maybe set the most significant bit and use the 7 remaining bits to encode which kind of boundary is represented. this would limit us to 128 values for the rest.

	cluster		independent of the others
	line		implies a page break
	page		implies a line break

===

facets: type (inscription or manuscript), date, place, language, script, repository

each searchable field must have a predefined type; more accurately, a processor name, which must define how to process the field to make it searchable (optionally indexable). use static typing, same processor for a given field in a given collection of fields.

for selecting snippets. for simplicity, try to select complete (balanced) block elements. but also need to cap the length of a snippet, to avoid degenerate cases with overlong paragraphs. we can abbreviate a block with '[...]' in appropriate spots. will need to define rules for deciding where exactly we can split a block.

but the block/inline distinction is not enough for the phys display. for consecutive lines should be part of a single search unit, but be displayed separately. so in fact we should have a "search block" unit in addition to display block units. but for now just support searching the logical display.

when processing div headings, add an infinite amount of space before and after the title. there should also be an infinite amount of space between divisions (but not between paragraphs, list elements, and other kinds of blocks[?]). to represent infinite space, use a special Unicode character that cannot appear in the text; but won't work with TRE and if we use wildcards; could patch TRE to bail out when it finds some given character, but quite dirty. in fact, no, that's just like using \n for line-based matching; we could even just use \n for separating all records; see agrep's -e option ("set the record delimiter regular expression"). when we have nested divs, just flatten the hierarchy, doesn't matter.

in fact, for matching, we could just have a sequence of lines (paragraphs) and not allow matches to cross lines, like grep. for the physical display, should just keep lines in a single block upto the next div, if any.

we might also want to treat the base text as a single string, with no gap between divisions. we should start with that: just make the base text searchable, not titles; use '\n' for separating paragraphs.

if we want to add an index, it should index the search representation, not the generated intermediary XML.

the search representation needs:

¶ an element that represents stuff that should be displayed but not made searchable. call it <nosearch> or sth. per default, everything is made searchable.

¶ clear distinction between "block" and "inline" elements;

so we need to care about:

	<div>
		(only textparts, need to find another mechanism for funky divs in criteds)
	<heading>
		...
	<span> (different variations; use classes systematically)
		...
	[other block]
		<p>, etc.
	[other inline]
		<i>, etc.

for highlighting and snippets to work, must be able to do matching and html conversion simultaneously; iow, it must be possible to go from a search offset to a display offset. but:

* a single search unit might represent several display units, if we have e.g.
  {dh}{a}{r}{m}{a} -> dharma
* a sequence of display units might not have a search unit, e.g. hyphens might need to be
  stripped or even spaces. in this case, should not select the omitted units unless they
  appear between two consecutive search units, so with display: "-a-b-c-" and query
  "ab" should select "-{a-b}-c-".
* can we have several search units matching a single display unit? I can only
  think of gaiji stuff, if we allow searching (parts of) gaiji symbol names.
  not sure we need this functionality though.

would be faster to have a single string for each document, so that we can perform a single scan of everything, but this is more complicated, so, for now, just store fields separately in the db.

===

will things be fast enough for basic re search? assume that yes

chercher une représentation unique pour les métadonnées en général pour le
texte, doit être en mesure de chercher deux représentations: physical+logical
que garder de la structure du texte? at least: titres, sauts de paragraphes (on
considère les éléments d'une liste comme des paragraphes),

to make regex matching faster can use some msgpack-like format. we should be able to
search the whole document in one go. we can assume that there is a fixed set of
fields, so no need to store the field name.

need to have some custom encoding(s), but can do that later on.
when encoding is done, might want to modify the library, for:
1. simplifying search. could be useful for enabling/disabling diacritics
   matching if we encode this info in each char.
2. make matching faster (we can avoid useless memory allocs).

Should we do the search stuff in Python, go or C? Since we are going to query
the db, might be simpler to have a SQLite extension, but then we must write
at least the binding code in C. For testing, it would be better to have an
interface that accepts commands, separate from sqlite, should have a cmd line
interface. It's probably best to
do everything through a sqlite extension, even if it's annoying, because this
way we're protected from transaction issues; but this requires us to store stuff
in sqlite tabvles.

Write the main code in python first. Then switch to go if not fast enough. Add a
basic argc,argv func as an entry point, and wrap this function in C with minimum
code.

should do all the text preprocessing stuff in python, including for modern
languages, tokenization, because it's where all libraries are. we can use some
binary format to indicate to the engine what it should index. only the matching
engine should be in C or Go. could in fact do all the indexing in python, and
the search stuff in C/Go.

In fact we can't really do auxiliary stuff like highlighting in anything else
than python, because need to navigate XML stuff. So should probably just write
everything in python and deal with slowness later on.

probably useful transfos:

  space-insensitive
  case-insensitive
  diacritics-insensitive viz. ASCII for phone search
  remove aspiration (dh>d, th>t, etc.)
  folding several into one e.g. vv>v ddh>dh
  folding one into several e.g. œ>oe æ>ae β>ss

still, we should start with a simple substring search function that searches the
exact text representation, and add a function for highlighting the stuff it
matches. then, handle hyphenated words, <unclear>, etc. and adjust
highlighting accordingly. only then, start to do sth interesting search-wise.

for indexing:

¶ in python, transform the xml into the search representation. do so in such a
way that we can map the selected text to an offset in the XML representation. iow, for each string we emit, we must know its boundaries in the search representation. don't use XML for the output because no XML parser in go, only a module for marshalling data structures (and because XML requires escaping). use a simple textual format. don't use formats that use escape sequences, to make sure we get proper offsets back. msgpack would do, too.

.field <TAB> title
.text <TAB> here one paragraph or sentence or phrase or single value
.text <TAB> another one
...
.field <TAB> edition.physical
.text <TAB> ...

prefix each string with its length. the search tool should use a single-byte
encoding, for commodity in the matching code. we could also use bits to encode
diacritics, case differences, whether aspirate or not. if this is faster than
just scanning the string. what shall we do with extra chars? just ignore them? or replace them with some 0xff?

each phrase should be numbered. this way, the search tool can return offsets of
the form start_phrase_no:start_offset TO end_phrase_no:end_offset, for
readability. and we only need to process a single phrase for highlighting the match.

on the python side, use xpath rather than offsets for locating paras nodes. (in fact,
should do that everywhere in the code.)

===

for regex matching, there is also https://intel.github.io/hyperscan/dev-reference/intro.html which can do approx matching (hamming+levenshtein). could be useful as an optimization. and there are python bindings.

and the regex python library also supports approx matching, based on tre's code apparently (see github issues one the subject).

===

from the wikipedia iso 15919 chart, we use:

a ā i ī u ū ŭ
r̥ r̥̄ l̥ (no l̥̄)
e ē ai o ō au
ṁ m̐ ḥ ẖ ḫ
ḵ
k kh g gh ṅ
c ch j jh ñ
ṭ ṭh ḍ ḍh ṇ
t th d dh n
p ph b bh m
ṟ
ṉ
ḻ
y r l ḷ v
ś ṣ s
h
'
q

---

punctuation: |, || (and convert the nagari equivalents to this)
 '‖' = double vertical line

---

we have the i in sajĭvakāla, encoding error?
as well the a in prăṇa, encoding error?
ạ ṙ
one occ. of ḹ, legit consonant?


do we have a copy od the standard

---

[0-9];
fractions!


===

for the edited text

convert the text to nfd? tokenize by grapheme cluster (with the unicode algorithm). then by phoneme. each phoneme should have start and end offsets viz. should be a single token. this way, we can easily select matches (skipping hyphens or other ignorable characters).

ignore non-starters that we do not recognize. thus if we have some alpha char plus diacritics, skip its diacritics and just emit the char. for starters that we don't recognize, use equivalence classes.

¶ keep other ascii letters as is (but without diacritics)
¶ characters that can occur within a cluster and that don't break this cluster into pieces; if they are letter, to be folded to a single character; if hyphens, etc., just skip them
¶ characters that always indicate cluster boundaries. could add assertions in tre to check whether we are at the beginning/end of a cluster, but might be simpler to implement if we add spaces or other special characters to represent delimitors.

colons are used for indicating a hiatus, as in pra:uga and ca:uttho and da:iā. but treat them as separators if they are followed by a space or EOS. can also be praüga and caüttho and daïā. can also be praUga and caUttho and daIĀ. (capital = initial vowel per contrast with vowel marks)

REPR p 11

capitals can be used for the same purpose, thus praUga. always fold capitals to lowercase.

spaces are supposed to be used as much as possible. but this might not be the case in practice.

hyphens are optional and are used for several purposes, not just for marking members of compounds, so just ignore them when processing the text.

deal with the avagraha ['’]. when ['’] occurs just before a whitespace character or at the end of a string,  don't treat it like an avagraha.

in tamil, a ['’] before a space or at the end of a string represent an elided 'u'

ṛ and ṝ and the equivalent with a ring are not ambiguous. but not so for ḷ and ḹ.

ṃ adn ṁ and m̐ (anunāsika) to anusvāra

convert ḫ (upadhmānīya) and ẖ (jihvāmūlīya) to visarga

tamil: @manu treatment of ḵ and ḷ and ṟ and ḻ (guide section 3 "Alphabet characters")

cam: m̃ to just anusvāra

[latin small letter schwa]
[latin small letter turned e]
[CYRILLIC SMALL LETTER IE]
vowel short schwa: fold ə and Ə and the cyrillic variants (Ә and ә) and ĕ
vowel long schwa: fold ə̄ and so on and also ə: and so on

??? for Mon and Pyu: ḅ and ṃ

e/ē and o/ō

ignore middle dots (zero vowel marker)

vowel support: q + vowel -> vowel

XXX nto sure for vowel support

REPR À P.19


===

fts5 would be ok for modern languages. but we will still need to add some stuff.
* custom fts5 tokenizer with stemmers from libstemmer
* for faceting, if we use traditional builtin sqlite stuff, we will have to
  issue a query two times, once for retrieving matching documents, the second
  time to retrieve aggregate data. might be slow, particularly if we use
  regexes.
* there is a NEAR operator, be we can't get an ordered window, would need to
  implement that
* phrase matches can be restricted to a paragraph by adding an EOP token, but
  this will impact ranking, and furthermore we can't do things like "find these
  2 phrases in the same paragraph".
